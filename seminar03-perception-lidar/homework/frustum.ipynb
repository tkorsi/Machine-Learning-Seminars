{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4246423f-5625-465c-94e5-a20c7dac5dff",
      "metadata": {
        "id": "4246423f-5625-465c-94e5-a20c7dac5dff"
      },
      "source": [
        "# Homework 3 - Frustum PointNet for 3D Object Detection\n",
        "![image](http://stanford.edu/~rqi/frustum-pointnets/images/teaser.jpg)\n",
        "\n",
        "В рамках этого домашнего задания вам надо будет реализовать архитектуру Frustum PointNet для 3D детекции объектов. Для выполнения очень рекомендуется машина с какой-то GPU.\n",
        "\n",
        "Референсная статья - [Frustum PointNets for 3D Object Detection from RGB-D Data](https://arxiv.org/abs/1711.08488)\n",
        "\n",
        "Этот метод использует как вход и лидарное облако, и картинку, но внутри декомпозирует задачу детекции объекта в 3D на две подзадачи, каждая из которых работает только с одним типом данных:\n",
        "- задетектировать объект в 2D на картинке, используя какой-либо готовый детектор объектов;\n",
        "- предсказать 3д параметры (положение, ориентацию, размеры) по куску лидарного облака, которое при репроекции в картинку попало бы в 2D бокс объекта.\n",
        "\n",
        "Для решения первой подзадачи возьмем готовый 2D детектор объектов из зоопарка моделей torchvision. Для решения второй задачи потребуется обучить две сетки:\n",
        "- pointnet для сегментации лидарных облаков. Он будет принимать на вход лидарные точки, проецирующиеся в 2D бокс (т.н frustum - усеченная пирамида), и сегментировать их на два класса: принадлежащие объекту или являющиеся фоном;\n",
        "- pointnet для регрессии параметров бокса. Он принимает на вход лидарные точки, которые были отсегментированны предыдущей сеткой как принадлежащие объекту, и возвращает параметры 3D бокса - координаты центра, размеры и ориентацию.\n",
        "\n",
        "Сети будем обучать последовательно. Для ускорения сеть будет обучаться на подмножестве датасета [kitti](https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d).\n",
        "\n",
        "Для получения полного балла за домашку надо будет дописать недостающий код, обучить сетки и побить бейзлайн.\n",
        "\n",
        "Перед решением домашки рекомендуется посмотреть третий семинар, во второй части которого рассказывается про математику проецирования лидарных точек в картинку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "864f8869-db90-4188-b1c9-58fa5bf8370d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "864f8869-db90-4188-b1c9-58fa5bf8370d",
        "outputId": "3a5f20b2-a50e-482f-ef4c-4e95c229d976"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6f9f1817-5dd7-40e4-9395-732f836a96f3",
      "metadata": {
        "id": "6f9f1817-5dd7-40e4-9395-732f836a96f3"
      },
      "outputs": [],
      "source": [
        "# check imports; if something fails, install it via pip3\n",
        "import shapely\n",
        "import scipy.optimize\n",
        "import plotly\n",
        "import matplotlib\n",
        "import sklearn.metrics  # pip3 install scikit-learn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb3e7be-c065-4fd7-b34d-2d24a3b7edd1",
      "metadata": {
        "id": "cbb3e7be-c065-4fd7-b34d-2d24a3b7edd1"
      },
      "source": [
        "## 1. Архитектуры PointNet'ов для сегментации и регрессии (2 балла)\n",
        "\n",
        "![image](https://stanford.edu/~rqi/pointnet/images/pointnet.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "87b65d99-20dc-4368-beeb-31cb8baa4e59",
      "metadata": {
        "id": "87b65d99-20dc-4368-beeb-31cb8baa4e59"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138c9a8d-92d0-4482-9660-a27378b382ee",
      "metadata": {
        "id": "138c9a8d-92d0-4482-9660-a27378b382ee"
      },
      "source": [
        "Класс `SpatialTransformerNetworkKDim` - реализация spatial transformer network (T-net со схемы выше) для предсказания как надо трансформировать облако (входное с координатами xyz или уже переведенное в некоторое новое пространство 1D свертками) в некоторое стандартизованное представление, из которого будет проще решать целевую задачу (см. рассказ про pointnet в лекции или [статью про pointnet](https://arxiv.org/abs/1612.00593))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1a5d3e26-078d-4702-a90e-2566c658b2fd",
      "metadata": {
        "id": "1a5d3e26-078d-4702-a90e-2566c658b2fd"
      },
      "outputs": [],
      "source": [
        "class SpatialTransformerNetworkKDim(nn.Module):\n",
        "    def __init__(self, k=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.fc1 = nn.Linear(1024, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, k*k)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.k = k\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x - tensor of shape [bs, self.k, n_points]\n",
        "        returns tensor of shape [bs, self.k, self.k] with transforms for each sample\n",
        "        \"\"\"\n",
        "        batchsize = x.size()[0]\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]  # maxpool\n",
        "        x = x.view(-1, 1024)\n",
        "\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = F.relu(self.bn5(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        identity_transform = Variable(\n",
        "            torch.from_numpy(np.eye(self.k, dtype=np.float32).flatten())).view(1, self.k*self.k).repeat(batchsize, 1)\n",
        "        if x.is_cuda:\n",
        "            identity_transform = identity_transform.cuda()\n",
        "        x = x + identity_transform\n",
        "        x = x.view(-1, self.k, self.k)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f990093-6def-427f-b9d5-02397c624f93",
      "metadata": {
        "id": "6f990093-6def-427f-b9d5-02397c624f93"
      },
      "source": [
        "`PointNetFeatureExtractor` - класс, реализующий основной backbone для вытаскивания фичей из облака. Выдает либо глобальный вектор фичей для облака (для задач регрессии и классификации; при `global_feat= True`), либо фичи для каждой точки (для задачи сегментации; при `global_feat = False`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cd8cf2aa-ab65-4a3b-8689-d43b87aec740",
      "metadata": {
        "id": "cd8cf2aa-ab65-4a3b-8689-d43b87aec740"
      },
      "outputs": [],
      "source": [
        "class PointNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, global_feat=True, feature_transform=False):\n",
        "        super().__init__()\n",
        "        self.stn = SpatialTransformerNetworkKDim(k=4)\n",
        "        self.conv1 = torch.nn.Conv1d(4, 64, 1)\n",
        "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
        "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(1024)\n",
        "        self.global_feat = global_feat\n",
        "        self.feature_transform = feature_transform\n",
        "        if self.feature_transform:\n",
        "            self.fstn = SpatialTransformerNetworkKDim(k=64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x - tensor of shape [bs, 4, n_points]\n",
        "        Returns tuple (features, input_stn_transform, interm_stn_transform), where\n",
        "        - features - either global features for pointcloud or features for each point\n",
        "        - input_stn_transform, interm_stn_transform - predicted transforms from SpatialTransformerNetworkKDim\n",
        "        \"\"\"\n",
        "        n_pts = x.size()[2]\n",
        "        trans = self.stn(x)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = torch.bmm(x, trans)\n",
        "        x = x.transpose(2, 1)\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        if self.feature_transform:\n",
        "            trans_feat = self.fstn(x)\n",
        "            x = x.transpose(2,1)\n",
        "            x = torch.bmm(x, trans_feat)\n",
        "            x = x.transpose(2,1)\n",
        "        else:\n",
        "            trans_feat = None\n",
        "\n",
        "        pointfeat = x\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x = torch.max(x, 2, keepdim=True)[0]\n",
        "        x = x.view(-1, 1024)\n",
        "        if self.global_feat:\n",
        "            return x, trans, trans_feat\n",
        "\n",
        "        x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
        "        return torch.cat([x, pointfeat], 1), trans, trans_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "09163e1f-b4a8-4ff2-b02f-4cb57d8dc1b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09163e1f-b4a8-4ff2-b02f-4cb57d8dc1b9",
        "outputId": "71f20cb1-c76a-4872-b887-15283776beff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global feat torch.Size([32, 1024])\n",
            "point feat torch.Size([32, 1088, 2500])\n"
          ]
        }
      ],
      "source": [
        "sim_data = Variable(torch.rand(32,4,2500))\n",
        "pointfeat = PointNetFeatureExtractor(global_feat=True)\n",
        "out, _, _ = pointfeat(sim_data)\n",
        "print('global feat', out.size())\n",
        "\n",
        "pointfeat = PointNetFeatureExtractor(global_feat=False)\n",
        "out, _, _ = pointfeat(sim_data)\n",
        "print('point feat', out.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57981296-5924-4098-8100-eacf2cf21471",
      "metadata": {
        "id": "57981296-5924-4098-8100-eacf2cf21471"
      },
      "source": [
        "`SegmentationPointNet` - сегментационный PointNet. Принимает на вход облако и возвращает логарифмированные вероятности за классы для каждой точки.\n",
        "\n",
        "**(1 балл)** допишите недостающие куски"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bda44d7-0e01-4153-9a7b-40574c09f99c",
      "metadata": {
        "id": "3bda44d7-0e01-4153-9a7b-40574c09f99c"
      },
      "outputs": [],
      "source": [
        "class SegmentationPointNet(nn.Module):\n",
        "    def __init__(self, k = 2, feature_transform=False):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.feature_transform = feature_transform\n",
        "        self.feat = PointNetFeatureExtractor(global_feat=False, feature_transform=feature_transform)\n",
        "        # ============YOUR CODE ===================\n",
        "        # define subnetwork that will transform point features from PointNetFeatureExtractor\n",
        "        # into k dims. it should be like (Conv1dBnRelu(1088, 512) + Conv1dBnRelu(512, 256) + Conv1dBnRelu(256, 128) + Conv1d(128, k))\n",
        "        raise NotImplementedError()\n",
        "        # ==========================================\n",
        "        self.logits = torch.nn.Conv1d(128, self.k, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x - pointcloud, tensor of shape [batchsize, n_points, 4]\n",
        "        \"\"\"\n",
        "        x = torch.moveaxis(x, -1, -2)  # swap 'dim' and 'points' axis\n",
        "        x, trans, _ = self.feat(x)\n",
        "        # ======= YOUR CODE HERE ====================\n",
        "        # apply your subnetwork to x\n",
        "        x = ...\n",
        "        raise NotImplementedError()\n",
        "        # ===========================================\n",
        "        x = self.logits(x)\n",
        "        return x, trans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca5df02c-6385-45ca-b302-320ba4807282",
      "metadata": {
        "id": "ca5df02c-6385-45ca-b302-320ba4807282"
      },
      "outputs": [],
      "source": [
        "sim_data = Variable(torch.rand(32,2500, 4))\n",
        "seg = SegmentationPointNet(k = 3)\n",
        "out, _ = seg(sim_data)\n",
        "print('seg', out.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "756f4a12-15fe-4340-9fe2-f079a93a91b3",
      "metadata": {
        "id": "756f4a12-15fe-4340-9fe2-f079a93a91b3"
      },
      "source": [
        "`PointNetDetector` - PointNet для предсказания параметров 3D бокса. Принимает на вход облако с точками, которые сегментационная сеть определила как часть объекта. Параметры центра коробки будем предсказывать напрямую. Параметры размеров и ориентации коробки будем предсказывать как классификацию бина с характерными параметрами и регрессию поправок к параметрам в бине.\n",
        "\n",
        "**(1 балл)** допишите недостающие куски"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feb04b42-bbf3-4ef6-826f-5406587af4a1",
      "metadata": {
        "id": "feb04b42-bbf3-4ef6-826f-5406587af4a1"
      },
      "outputs": [],
      "source": [
        "class PointNetDetector(nn.Module):\n",
        "    def __init__(self, feature_transform=False, num_heading_bins=12, num_size_clusters=8):\n",
        "        super(PointNetDetector, self).__init__()\n",
        "        self.feature_transform = feature_transform\n",
        "        self.feat = PointNetFeatureExtractor(global_feat=True, feature_transform=feature_transform)\n",
        "        # ============YOUR CODE ===================\n",
        "        # define subnetwork that will transform global features for cloud from PointNetFeatureExtractor\n",
        "        # into final 3d box parameters.\n",
        "        # it should be like (Linear(1024, 512) + BN + Relu + Linear(512, 256) + Dropout(0.3) + BN + Relu + heads)\n",
        "        raise NotImplementedError()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        # ==========================================\n",
        "\n",
        "        self.center_reg_head = nn.Linear(256, 3)\n",
        "\n",
        "        self.size_class_head = nn.Linear(256, num_size_clusters)\n",
        "        self.size_reg_head = nn.Linear(256, num_size_clusters * 3)\n",
        "\n",
        "        self.heading_class_head = nn.Linear(256, num_heading_bins)\n",
        "        self.heading_reg_head = nn.Linear(256, num_heading_bins)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x - pointcloud, tensor of shape [batchsize, n_points, 4]\n",
        "        \"\"\"\n",
        "        x = torch.moveaxis(x, -1, -2)  # swap 'dim' and 'points' axis\n",
        "\n",
        "        x, trans, _ = self.feat(x)\n",
        "        # ======= YOUR CODE HERE ====================\n",
        "        # apply your subnetwork to x\n",
        "        x = ...\n",
        "        # ===========================================\n",
        "        center_reg = self.center_reg_head(x)\n",
        "        size_class = self.size_class_head(x)\n",
        "        size_reg = self.size_reg_head(x)\n",
        "        heading_class = self.heading_class_head(x)\n",
        "        heading_reg = self.heading_reg_head(x)\n",
        "\n",
        "        return center_reg, size_class, size_reg, heading_class, heading_reg, \\\n",
        "                trans  # feature transform regularization goes here\n",
        "\n",
        "\n",
        "def feature_transform_regularizer(trans):\n",
        "    \"\"\"\n",
        "    trans - tensor of shape [bs, d, d] with transforms of d-dim points into new space\n",
        "    regularizer forces 'trans' to be close to orthonormal i.e trans.dot(tran.T) == I\n",
        "    \"\"\"\n",
        "    d = trans.size()[1]\n",
        "    batchsize = trans.size()[0]\n",
        "    I = torch.eye(d)[np.newaxis]\n",
        "    if trans.is_cuda:\n",
        "        I = I.cuda()\n",
        "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f880709-e911-4766-b49f-3ed6c7808be5",
      "metadata": {
        "id": "7f880709-e911-4766-b49f-3ed6c7808be5"
      },
      "outputs": [],
      "source": [
        "print('stn: ')\n",
        "sim_data_stn_4d = Variable(torch.rand(32,4, 2500))\n",
        "trans = SpatialTransformerNetworkKDim(k=4)\n",
        "out = trans(sim_data_stn_4d)\n",
        "print('stn out', out.size())\n",
        "print('loss', feature_transform_regularizer(out))\n",
        "\n",
        "print('\\n stn 64d: ')\n",
        "sim_data_stn_64d = Variable(torch.rand(32, 64, 2500))\n",
        "trans = SpatialTransformerNetworkKDim(k=64)\n",
        "out = trans(sim_data_stn_64d)\n",
        "print('stn64d out', out.size())\n",
        "print('loss', feature_transform_regularizer(out))\n",
        "\n",
        "print('\\n detector: ')\n",
        "sim_data = Variable(torch.rand(32,2500, 4))\n",
        "detector = PointNetDetector()\n",
        "out = detector(sim_data)\n",
        "print('center_reg', out[0].size())\n",
        "print('size class', out[1].size())\n",
        "print('size reg', out[2].size())\n",
        "print('heading class', out[3].size())\n",
        "print('heading reg', out[4].size())\n",
        "print('input transform', out[5].size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10bcc60b-b925-4aff-a466-73a27e27b0f4",
      "metadata": {
        "id": "10bcc60b-b925-4aff-a466-73a27e27b0f4"
      },
      "source": [
        "## 2. Подготовка данных для обучения (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cedc268b-9919-4a9b-ad03-3c3c882338b9",
      "metadata": {
        "id": "cedc268b-9919-4a9b-ad03-3c3c882338b9"
      },
      "source": [
        "Загрузим сабсэт kitti"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec1c0a7-9556-4487-b032-471f652e4b11",
      "metadata": {
        "id": "2ec1c0a7-9556-4487-b032-471f652e4b11"
      },
      "outputs": [],
      "source": [
        "from yfile import download_from_yadisk\n",
        "import os\n",
        "TARGET_DIR = '/home/vyurchenko/data/shad/kitti_subset'\n",
        "FILENAME = \"kitti_subset.zip\"\n",
        "if not os.path.exists(os.path.join(TARGET_DIR, FILENAME)):\n",
        "    # we are going to download 2.7 gb file, downloading will take some time\n",
        "    download_from_yadisk(\n",
        "        short_url='https://disk.yandex.ru/d/7fdMyxhreW_SiA',\n",
        "        filename=FILENAME,\n",
        "        target_dir=TARGET_DIR\n",
        "    )\n",
        "    # alternative way:\n",
        "    #from gfile import download_list\n",
        "    #download_list(url=https://drive.google.com/file/d/1VdLawplRcdT3UfvwNsihgzjJ0ks-Cmn0,\n",
        "    #               filename=FILENAME, target_dir=TARGET_DIR)\n",
        "filesize = os.path.getsize(os.path.join(TARGET_DIR, FILENAME))\n",
        "GB = 2**30\n",
        "assert filesize > 1 * GB, f\"{filesize} is too small, something wrong with downloading\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c41824be-1fb3-4abd-820f-03ef04904045",
      "metadata": {
        "id": "c41824be-1fb3-4abd-820f-03ef04904045"
      },
      "outputs": [],
      "source": [
        "! unzip -q kitti_subset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac7b2ae7-e3a4-4344-9155-734a5e0f3789",
      "metadata": {
        "id": "ac7b2ae7-e3a4-4344-9155-734a5e0f3789"
      },
      "source": [
        "Класс для представления датасета KITTI реализован за вас. По сути он хранит внутри сэмплы из датасета в сыром виде. Описание датасета доступно в файле `KITTI dataset description.txt`, который лежит в архиве с датасетом.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e657292d-5383-4d0e-8548-9f29f8b0bbef",
      "metadata": {
        "id": "e657292d-5383-4d0e-8548-9f29f8b0bbef"
      },
      "outputs": [],
      "source": [
        "from kitti_dataset import split_train_val, KittiDataset\n",
        "\n",
        "KITTI_ROOT = os.path.join(TARGET_DIR, \"kitti\")\n",
        "\n",
        "train_items, val_items = split_train_val(KITTI_ROOT, fraction=0.25)\n",
        "\n",
        "train_kitti = KittiDataset(KITTI_ROOT, items=train_items, split='training', only_easy=False)\n",
        "val_kitti = KittiDataset(KITTI_ROOT, items=val_items, split='training', only_easy=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb5dd68-8781-4723-9022-560264d59dc6",
      "metadata": {
        "id": "deb5dd68-8781-4723-9022-560264d59dc6"
      },
      "source": [
        "Каждый сэмпл датасета хранит картинку, лидарное облако, структуру с калибровками, структуру с gt данными детектора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f065242-645b-41d1-8bc3-18d9a312b9bb",
      "metadata": {
        "scrolled": true,
        "id": "6f065242-645b-41d1-8bc3-18d9a312b9bb"
      },
      "outputs": [],
      "source": [
        "train_kitti[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4256f6d4-a4a9-4b82-8eda-25c49b96e8e9",
      "metadata": {
        "id": "4256f6d4-a4a9-4b82-8eda-25c49b96e8e9"
      },
      "source": [
        "Для того, чтобы сформировать датасет для обучения детектора потребуется дописать два класса: `Projector` и `Detector2DWrapper`.\n",
        "\n",
        "Класс `Projector` реализует преобразования между различными фреймами. Вам нужно научиться трансформировать данные между фреймом камеры и лидара, а также проецировать точки на изображение и точки с изображения -- в лучи во фрейме камеры. Подробнее про формат данных можно прочитать в https://www.cvlibs.net/datasets/kitti/setup.php, https://towardsdatascience.com/kitti-coordinate-transformations-125094cd42fb\n",
        "\n",
        "Для трансформаций облаков потребуется использовать класс `Calibration`, который имеет интерфейс dict'a. Из него вам потребуются следующие поля:\n",
        "- `Tr_velo_to_cam` - матрица преобразований из системы координат лидара в систему координат камеры\n",
        "- `R0_rect` - матрица ректификации изображения - преобразование точек из системы координат камеры в некоторую новую систему, учитывающую искажения линзы (дисторсию).\n",
        "- `P2` - матрица проекции точек в системе координат камеры (с учтенной дисторсией) на плоскость картинки\n",
        "Таким образом чтобы спроецировать точки, заданные в системе координат камеры, на плоскость картинки, надо их сначала домножить на матрицу `R0_rect` , а потом на матрицу `P2`\n",
        "\n",
        "- **(1 балл)** Допишите преобразования между различными фреймами в класс `Projector`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ede8662-de3e-42a2-a60e-ad54a330efaa",
      "metadata": {
        "id": "6ede8662-de3e-42a2-a60e-ad54a330efaa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "from kitti_dataset import Calibration\n",
        "\n",
        "\n",
        "class Projector:\n",
        "    @staticmethod\n",
        "    def to_homogenous_coords(coords_3d: NDArray) -> NDArray:\n",
        "        \"\"\"\n",
        "        @param coords_3d: (..., K)\n",
        "        @return: NDArray of same dtype with shape (..., K + 1) -- same points in homogenous coordinates\n",
        "\n",
        "        \"\"\"\n",
        "        assert coords_3d.shape[-1] < 4, \"In this task, this function should never be called with last dimention >= 4\"\n",
        "        #===========YOUR CODE==========================\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @staticmethod\n",
        "    def from_homogenous_coords(coords_3d: NDArray) -> NDArray:\n",
        "        \"\"\"\n",
        "        @param coords_3d: (N, K), where last dimention corresponds to homogenous dimention\n",
        "        @return: NDArray of same dtype with shape (N, K - 1) -- same points in homogenous coordinates\n",
        "\n",
        "        \"\"\"\n",
        "        assert coords_3d.shape[-1] > 2\n",
        "        #===========YOUR CODE==========================\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @staticmethod\n",
        "    def _check_and_maybe_cast_to_homogenous(coords):\n",
        "        assert coords.shape[-1] in (3, 4)\n",
        "        if coords.shape[-1] == 3:\n",
        "            coords = Projector.to_homogenous_coords(coords)\n",
        "        return coords\n",
        "\n",
        "    @staticmethod\n",
        "    def world_to_camera(world_coords, calib: Calibration):\n",
        "        \"\"\"\n",
        "        This function projects points from world coorinate frame to camera coordinate frame\n",
        "        Note: check that you do not pass Velodyne points with intensity here, as intensity occupies 4th dimention as well\n",
        "\n",
        "        @param world_coords: (..., 3) or (..., 4); if not homogenous, will be converted to homogenous\n",
        "        @return: camera frame coordinates, homogenous\n",
        "        \"\"\"\n",
        "        world_coords = Projector._check_and_maybe_cast_to_homogenous(world_coords)\n",
        "        #===========YOUR CODE==========================\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @staticmethod\n",
        "    def camera_to_world(camera_coords, calib: Calibration):\n",
        "        \"\"\"\n",
        "        @param camera_coords: (..., 3) or (..., 4); if not homogenous, will be converted to homogenous\n",
        "        @return: world frame coordinates, homogenous\n",
        "        \"\"\"\n",
        "        camera_coords = Projector._check_and_maybe_cast_to_homogenous(camera_coords)\n",
        "        #===========YOUR CODE==========================\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @staticmethod\n",
        "    def camera_to_projection(camera_coords, calib: Calibration):\n",
        "        \"\"\"\n",
        "        @param camera_coords: (..., 3) or (..., 4); if not homogenous, will be converted to homogenous\n",
        "        @return: xyw coordinates in image projection frame, shape (..., 3)\n",
        "        \"\"\"\n",
        "        camera_coords = Projector._check_and_maybe_cast_to_homogenous(camera_coords)\n",
        "        #===========YOUR CODE==========================\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @staticmethod\n",
        "    def _pad_matrix_to_44(mtx):\n",
        "        padded = np.eye(4, dtype=mtx.dtype)\n",
        "        padded[:mtx.shape[0], :mtx.shape[1]] = mtx\n",
        "        return padded\n",
        "\n",
        "    @staticmethod\n",
        "    def projection_to_camera(projected_coords, calib: Calibration):\n",
        "        \"\"\"\n",
        "        @param projection_coords: (..., 3) -- xyw\n",
        "        @return: homogenous coordinates in camera frame, shape (..., 4)\n",
        "        \"\"\"\n",
        "        projected_coords = Projector._check_and_maybe_cast_to_homogenous(projected_coords)\n",
        "        return projected_coords @ np.linalg.inv(Projector._pad_matrix_to_44(calib['P2']) @ calib['R0_rect']).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a09553-2457-47e0-8245-af9ee1204f30",
      "metadata": {
        "id": "d1a09553-2457-47e0-8245-af9ee1204f30"
      },
      "source": [
        "Класс `Detector2DWrapper` - обертка над предобученным 2D детектором объектов на картинке из зоопарка моделей torchvision. Хотя в датасете доступны координаты объектов на картинке, будет некорректным учить frustum pointnet на \"идеальных\" кропах т.к. на этапе инференса они будут недоступны. Поэтому для обучения потребуется предварительно сопоставить результат 2D детектора с gt 2D боксами объектов и использовать для обучения наиболее близкий бокс из детектора. Эта логика должна быть реализована в методе `match_with_gt_objects`\n",
        "\n",
        "- **(1 балл)** реализуйте жадный матчинг gt объектов к предсказанным 2д боксам из детектора по iou в функции `match_with_gt_objects`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df8b2c8-7b2f-4e49-86dc-61c850f00326",
      "metadata": {
        "id": "3df8b2c8-7b2f-4e49-86dc-61c850f00326"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.models.detection as detection_zoo\n",
        "import numpy as np\n",
        "\n",
        "from typing import TypedDict, Union, Literal, List, Tuple\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "from kitti_dataset import Label\n",
        "\n",
        "\n",
        "class Detector2DWrapper:\n",
        "    def __init__(self):\n",
        "        self.camera_weights = detection_zoo.FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\n",
        "        self.camera_model = detection_zoo.fasterrcnn_resnet50_fpn_v2(weights=self.camera_weights, box_score_thresh=0.925)\n",
        "        self.camera_model.eval()\n",
        "        self.cuda = False\n",
        "        self.image_preprocess = self.camera_weights.transforms()\n",
        "\n",
        "    def evaluate(self, image) -> torch.FloatTensor:\n",
        "        \"\"\"\n",
        "        @param image: Tensor of shape (3, H, W), value range [0, 255]\n",
        "        @return: camera bboxes as tensor of shape (N, 4). Detections should be filtered to only contain\n",
        "        classes that are present in KITTI\n",
        "\n",
        "        \"\"\"\n",
        "        SELECTED_LABELS = ['person', 'bicycle', 'car', 'motorcycle', 'bus', 'truck']\n",
        "        image = (image / 255.0).float()\n",
        "        if self.cuda:\n",
        "            image = image.cuda()\n",
        "        with torch.no_grad():\n",
        "            image_batch = [self.image_preprocess(image)]\n",
        "            predictions = self.camera_model(image_batch)\n",
        "        prediction = {x: y.cpu() for x, y in predictions[0].items()}\n",
        "\n",
        "        # filter labels\n",
        "        mask = [\n",
        "            self.camera_weights.meta[\"categories\"][cat] in SELECTED_LABELS\n",
        "            for cat in prediction[\"labels\"]]\n",
        "        mask = torch.tensor(mask)\n",
        "        if not mask.size(0) or not mask.any():\n",
        "            return prediction['boxes']\n",
        "        prediction = {x: y[mask] for x, y in prediction.items()}\n",
        "        return prediction['boxes']\n",
        "\n",
        "    @staticmethod\n",
        "    def match_with_gt_objects(gt_objects: List[Label], cam_bboxes: torch.FloatTensor, iou_threshold=0.5):\n",
        "        \"\"\"\n",
        "        @param gt_objects: objects in KITTI scene\n",
        "        @param cam_bboxes: detections bboxes returned from Detector2DWrapper.evaluate for this KITTI scene\n",
        "        @param iou_threshold: minimum IOU for detection and GT entry to be considered matched\n",
        "\n",
        "        @return: entries of gt_objects, for which a matching camera detection was found. gt `bbox` field should be\n",
        "        replaced with found detection bbox. For each gt entry, pick the best (in terms of bbox IoU) detection\n",
        "        if their IoU is at least iou_threshold. Do not drop 'DontCare' labels here, we need them for metrics later.\n",
        "        If some gt object has no predicted object with iou higher than threshold, drop it.\n",
        "        \"\"\"\n",
        "\n",
        "        gt_bboxes = [x['bbox'] for x in gt_objects]\n",
        "        ans = []\n",
        "        # ============YOUR CODE========================\n",
        "        # you may use torchvision.ops.box_iou() to compute pairwise iou of gt_bboxes and cam_bboxes\n",
        "        raise NotImplementedError()\n",
        "        # ==============================================\n",
        "        return ans\n",
        "\n",
        "    def to_cuda(self):\n",
        "        self.camera_model = self.camera_model.cuda()\n",
        "        self.image_preprocess = self.image_preprocess.cuda()\n",
        "        self.cuda = True\n",
        "\n",
        "    def to_cpu(self):\n",
        "        self.camera_model = self.camera_model.cpu()\n",
        "        self.image_preprocess = self.image_preprocess.cpu()\n",
        "        self.cuda = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47f42df4-e73f-487a-b402-5f2158f8cddf",
      "metadata": {
        "id": "47f42df4-e73f-487a-b402-5f2158f8cddf"
      },
      "outputs": [],
      "source": [
        "detector_2d_wrapper = Detector2DWrapper()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25ceb6ae-2976-4728-b3a5-66bf6f3923b4",
      "metadata": {
        "id": "25ceb6ae-2976-4728-b3a5-66bf6f3923b4"
      },
      "source": [
        "Класс `FrustumDataset` реализован за вас, он принимает на вход датасет kitti, объекты классов Detector2DWrapper и Projector и возвращает необходимые для обучения данные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94f83c32-1861-4448-a373-29a133578f86",
      "metadata": {
        "id": "94f83c32-1861-4448-a373-29a133578f86"
      },
      "outputs": [],
      "source": [
        "from frustum_dataset import FrustumDataset\n",
        "\n",
        "frustum_train = FrustumDataset(train_kitti, detector_2d_wrapper, Projector(), cuda=True)\n",
        "frustum_val = FrustumDataset(val_kitti, detector_2d_wrapper, Projector(), cuda=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e650aae8-1ecc-48b1-8195-4c33b9f2c0dc",
      "metadata": {
        "id": "e650aae8-1ecc-48b1-8195-4c33b9f2c0dc"
      },
      "source": [
        "Каждый сэмпл датасета - это\n",
        "- frustum с облаком (поле 'cloud') выровненным т.ч. ось z проходила по центру кропа объекта, которому этот frustum соответствует\n",
        "- картинка с кропом (поле 'image'; нужна только для визуализаций, в обучении не используется),\n",
        "- gt сегментация облака (поле 'cloud_segmentation'),\n",
        "- закодированны размер бокса - поле `size_idx` с индексом класса, поле `size_residual` с поправками к размеру относительно типичных размеров в классах\n",
        "- закодированная ориентация бокса - поле `heading_idx` с индексом класса и поле `heading_residual` с поправками к центрам бинов с ориентацией\n",
        "- поле `world_location` c координатами центра бокса\n",
        "- индекс сцены в датасете kitti `kitti_scene_idx`\n",
        "- `frustrum_rotation_angle` - угол насколько frustum был повернут"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2aad711b-def8-46a2-a42b-34e13464cb7e",
      "metadata": {
        "scrolled": true,
        "id": "2aad711b-def8-46a2-a42b-34e13464cb7e"
      },
      "outputs": [],
      "source": [
        "frustum_train[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1617ddab-3172-4f81-8081-e34e37c8a956",
      "metadata": {
        "id": "1617ddab-3172-4f81-8081-e34e37c8a956"
      },
      "source": [
        "Повизуализируем gt данные для сегментации с помощью plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ccd08d0-ba2e-446c-a8ba-48a6fdce9227",
      "metadata": {
        "id": "5ccd08d0-ba2e-446c-a8ba-48a6fdce9227"
      },
      "outputs": [],
      "source": [
        "from plotly.offline import init_notebook_mode\n",
        "from plotly.offline import iplot\n",
        "from plotly.offline import plot\n",
        "from plotly import graph_objs as go\n",
        "init_notebook_mode()\n",
        "\n",
        "def plotly_add_cloud(fig, cloud, color, colorscale=None, name=None, showscale=None, cmin=None, cmax=None):\n",
        "    fig.add_scatter3d(x=cloud[..., 0], y=cloud[..., 1], z=cloud[..., 2], name=name, mode='markers',\n",
        "        marker=dict(size=2, color=color, colorscale=colorscale, showscale=showscale, cmin=cmin, cmax=cmax))\n",
        "\n",
        "\n",
        "fig = go.Figure(layout=dict(scene=dict(\n",
        "        aspectmode='data',\n",
        "        xaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "        yaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "        zaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "    ), plot_bgcolor='#000', paper_bgcolor='black'))\n",
        "\n",
        "frustum_scene = frustum_train[2]\n",
        "kitti_scene = train_kitti[3]\n",
        "\n",
        "plotly_add_cloud(fig, frustum_scene['cloud'], frustum_scene['cloud_segmentation'].astype(np.int32), colorscale='Reds', cmin=0, cmax=1, name='Velodyne')\n",
        "# plotly_add_cloud(fig, frustum_scene['object_cloud'], frustum_scene['object_cloud'][:, -1], colorscale='Reds', cmin=0, cmax=1, name='Velodyne')\n",
        "# plotly_add_cloud(fig, kitti_scene['cloud'], np.ones_like(kitti_scene['cloud'][:, 0]), colorscale='Blues', cmin=0, cmax=0, name='Velodyne')\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625c4f13-7c75-4cd4-b6bf-5bd1b7482e12",
      "metadata": {
        "id": "625c4f13-7c75-4cd4-b6bf-5bd1b7482e12"
      },
      "source": [
        "Для референса в датасете также доступны картинки, но для обучения они нам не понадобятся:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d284ff-7ae2-4a38-b89e-73a54be424e6",
      "metadata": {
        "id": "e3d284ff-7ae2-4a38-b89e-73a54be424e6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.imshow(torch.moveaxis(frustum_train.kitti_dataset[0]['image'], 0, -1))\n",
        "\n",
        "for x1, y1, x2, y2 in detector_2d_wrapper.evaluate(frustum_train.kitti_dataset[0]['image']):\n",
        "    plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], color='red', linewidth=2)\n",
        "\n",
        "for item in frustum_train.kitti_dataset[0]['labels']:\n",
        "    x1, y1, x2, y2 = item['bbox']\n",
        "    plt.plot([x1, x1, x2, x2, x1], [y1, y2, y2, y1, y1], color='green', linewidth=1.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ba83b49-6f04-4a1c-948e-7c05cde8b179",
      "metadata": {
        "id": "2ba83b49-6f04-4a1c-948e-7c05cde8b179"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 12))\n",
        "plt.imshow(torch.moveaxis(train_kitti[1]['image'], 0, -1))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Image is only available for visualization purposes; do not use it in network\n",
        "plt.figure()\n",
        "plt.imshow(torch.moveaxis(frustum_train[7]['image'], 0, -1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92a8f41c-1316-4346-aef0-458980ef26b6",
      "metadata": {
        "id": "92a8f41c-1316-4346-aef0-458980ef26b6"
      },
      "source": [
        "## 3. Обучение pointnet'a для сегментации облака (2 балла)\n",
        "\n",
        "Учим часть про сегментацию. Вам необходимо дописать часть про подсчет лосса и функцию получения предсказаний для метрик по результатам прогона сетки.\n",
        "\n",
        "В качестве лосса для сегментации используется `nn.functional.cross_entropy` и `feature_transform_regularizer` как регуляризатор на результат SpatialTransformerNetwork, использующийся внутри поинтнета для сегментации.\n",
        "\n",
        "В функции `get_predictions_for_metrics` на выходе ожидаются вероятности за класс 1 и gt метки для точек.\n",
        "\n",
        "**(1 балл)**: дописан недостающий код. Лосс уменьшается в ходе обучения, метрики вычисляются\n",
        "\n",
        "**(1 балл)**: сеть достигла хотя бы 0.96 ROC AUC на валидации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d25cb21d-6375-4ee6-8341-e91965e7d9e9",
      "metadata": {
        "id": "d25cb21d-6375-4ee6-8341-e91965e7d9e9"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics\n",
        "\n",
        "def calculate_segmentation_loss(gt, segmentation_output, transform_regularizer_weight):\n",
        "    \"\"\"\n",
        "    @return: tuple of (classification loss, regularization from pointnet). Both are scalars (tensors of empty shape)\n",
        "    \"\"\"\n",
        "    # ==========YOUR CODE GOES HERE==================\n",
        "    # don't forget to multiply regularizer_loss on transform_regularizer_weight\n",
        "    segm, trans = segmentation_output\n",
        "    segmentation_loss = ...\n",
        "    regularizer_loss = ...\n",
        "    return segmentation_loss, regularizer_loss\n",
        "\n",
        "\n",
        "def get_predictions_for_metrics(gt, segmentation_output):\n",
        "    \"\"\"\n",
        "    @return: tuple of (batch_pred, batch_true) -- for every point in every batch element, predicted probability and gt label.\n",
        "    Both tensors are of shape (batch_sz, NUM_POINTS)\n",
        "    \"\"\"\n",
        "    # ==========YOUR CODE GOES HERE==================\n",
        "    segm, trans = segmentation_output\n",
        "    positive_class_probs = ...\n",
        "    gt_labels = ...\n",
        "    return positive_class_probs, gt_labels\n",
        "\n",
        "\n",
        "class RunningMean:\n",
        "    def __init__(self):\n",
        "        self.cnt = 0\n",
        "        self.sum = 0\n",
        "\n",
        "    def add(self, value):\n",
        "        self.sum += value\n",
        "        self.cnt += 1\n",
        "\n",
        "    def get(self):\n",
        "        return 0 if self.cnt == 0 else self.sum / self.cnt\n",
        "\n",
        "\n",
        "def train_segmentation_pointnet_one_epoch(segmentation_pointnet, train_data_generator, optimizer, transform_regularizer_weight):\n",
        "    mean_segm_loss = RunningMean()\n",
        "    mean_reg_loss = RunningMean()\n",
        "\n",
        "    segmentation_pointnet.train()\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_data_generator):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch = {x: y.cuda() for x, y in batch.items()}\n",
        "        segmentation_output = segmentation_pointnet(batch['cloud'].float())\n",
        "\n",
        "        segm_loss, reg_loss = calculate_segmentation_loss(batch, segmentation_output, transform_regularizer_weight)\n",
        "        loss = segm_loss + reg_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        mean_segm_loss.add(segm_loss)\n",
        "        mean_reg_loss.add(reg_loss)\n",
        "\n",
        "        print(f'Train batch: {batch_idx:4d}/{len(train_data_generator)} segmentation: {mean_segm_loss.get():.3f}, '\n",
        "              f'segmentation trans reg: {mean_reg_loss.get():.3f}', end='\\r')\n",
        "    print()\n",
        "    return mean_segm_loss.get()\n",
        "\n",
        "\n",
        "def eval_segmentation_model(segmentation_pointnet, val_data_generator, transform_regularizer_weight):\n",
        "    mean_segm_loss = RunningMean()\n",
        "    mean_reg_loss = RunningMean()\n",
        "    with torch.no_grad():\n",
        "        segmentation_pointnet.eval()\n",
        "        Y_pred = []\n",
        "        Y_true = []\n",
        "        for batch_idx, batch in enumerate(val_data_generator):\n",
        "            batch = {x: y.cuda() for x, y in batch.items()}\n",
        "            segmentation_output = segmentation_pointnet(batch['cloud'].float())\n",
        "            segm_loss, reg_loss = calculate_segmentation_loss(batch, segmentation_output, transform_regularizer_weight)\n",
        "            batch_pred, batch_true = get_predictions_for_metrics(batch, segmentation_output)\n",
        "            Y_pred.append(batch_pred.cpu())\n",
        "            Y_true.append(batch_true.cpu())\n",
        "            mean_segm_loss.add(segm_loss)\n",
        "            mean_reg_loss.add(reg_loss)\n",
        "            print(f'Valid batch: {batch_idx:4d}/{len(val_data_generator)} segmentation: {mean_segm_loss.get():.3f}, '\n",
        "                  f'segmentation trans reg: {mean_reg_loss.get():.3f}', end='\\r')\n",
        "    print()\n",
        "    Y_pred = torch.cat(Y_pred)\n",
        "    Y_true = torch.cat(Y_true)\n",
        "    print(f'Valid AP: {sklearn.metrics.average_precision_score(Y_true, Y_pred):.3f}, '\n",
        "          f'ROC AUC: {sklearn.metrics.roc_auc_score(Y_true, Y_pred):.3f}' )\n",
        "    return mean_segm_loss.get()\n",
        "\n",
        "\n",
        "def train_segmentation_pointnet(segmentation_pointnet, frustum_train, frustum_val, batch_size=16, n_epochs=90,\n",
        "                                transform_regularizer_weight=1.0):\n",
        "    train_data_generator = data.DataLoader(frustum_train, batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    val_data_generator = data.DataLoader(frustum_val, batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    optim = torch.optim.Adam([\n",
        "            {'params': segmentation_pointnet.parameters()}\n",
        "        ], lr=1e-3, weight_decay=1e-6)\n",
        "    lr_sched = torch.optim.lr_scheduler.StepLR(optim, step_size=30, gamma=0.2)\n",
        "\n",
        "    best_model_loss = None\n",
        "    for epoch in range(n_epochs):\n",
        "        print('Epoch:', epoch)\n",
        "        train_segmentation_pointnet_one_epoch(segmentation_pointnet, train_data_generator, optim, transform_regularizer_weight)\n",
        "\n",
        "        model_loss = eval_segmentation_model(segmentation_pointnet, val_data_generator, transform_regularizer_weight)\n",
        "\n",
        "        if best_model_loss is None or model_loss < best_model_loss:\n",
        "            best_model_loss = model_loss\n",
        "            torch.save(segmentation_pointnet.state_dict(), 'segmentation.pth')\n",
        "            print('new best model is saved')\n",
        "        print()\n",
        "        lr_sched.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b51a75ae-6bb6-47b0-8b9e-96f10ae45d0d",
      "metadata": {
        "scrolled": true,
        "id": "b51a75ae-6bb6-47b0-8b9e-96f10ae45d0d"
      },
      "outputs": [],
      "source": [
        "segmentation_pointnet = SegmentationPointNet().cuda()\n",
        "train_segmentation_pointnet(segmentation_pointnet, frustum_train, frustum_val,\n",
        "                            batch_size=16, n_epochs=90,\n",
        "                            transform_regularizer_weight=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8337e6e-6d4d-4463-ba46-4c4cde201bdf",
      "metadata": {
        "id": "e8337e6e-6d4d-4463-ba46-4c4cde201bdf"
      },
      "outputs": [],
      "source": [
        "segmentation_pointnet = SegmentationPointNet().cuda()\n",
        "segmentation_pointnet.load_state_dict(torch.load('segmentation.pth'))\n",
        "segmentation_pointnet.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "773e53c0-770d-49b1-8367-370fea80c7ac",
      "metadata": {
        "id": "773e53c0-770d-49b1-8367-370fea80c7ac"
      },
      "source": [
        "## 4. Обучение pointnet'a для детекции (4 балла)\n",
        "\n",
        "В этом блоке предстоит дописать код в функциях `calculate_detection_loss`, `cloud_to_object_cloud` и `detection_output_to_center_dims_rot`\n",
        "\n",
        "- **(1 балл)** Дописан весь недостающий код, обучение идет, лосс уменьшается\n",
        "- **(3 балла)**: сеть достигла\n",
        "  - хотя бы 0.80/0.80 precision/recall для автомобилей с min_iou=0.70\n",
        "  - хотя бы 0.80/0.80 precision/recall для пешеходов с min_iou=0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f292aee0-1727-49a1-81ba-f2c3f6b3cd95",
      "metadata": {
        "scrolled": true,
        "id": "f292aee0-1727-49a1-81ba-f2c3f6b3cd95"
      },
      "outputs": [],
      "source": [
        "from frustum_dataset import get_detection3d_corner_points, NUM_SIZE_CLUSTER, NUM_HEADING_BIN, g_mean_size_arr\n",
        "\n",
        "OBJECT_CLOUD_SZ = 512\n",
        "\n",
        "def calculate_detection_loss(gt, detection_output, transform_regularizer_weight):\n",
        "    center, size_class, size_reg, heading_class, heading_reg, stn_translation = detection_output\n",
        "\n",
        "    # ======== YOUR CODE =====================\n",
        "    # center loss - huber loss between gt['world_location'] and `center` prediction\n",
        "    # size_class_loss - cross-entropy between gt['size_idx'] and size_class\n",
        "    # heading_class_loss - cross-entropy between gt['heading_idx'] and heading_class\n",
        "    # regularizer_loss - calc via feature_transform_regularizer (don't forget to multiply it on transform_regularizer_weight)\n",
        "    # size_reg_loss - huber loss between gt['size_residual'] and size_reg WHICH CORRESPOND TO GT CLASS gt['size_idx']\n",
        "    # heading_reg_loss - humber loss between gt['heading_residual'] and heading_reg WHICH CORRESPOND TO GT CLASS gt['heading_idx']\n",
        "\n",
        "    center_loss = ...\n",
        "    size_class_loss = ...\n",
        "    size_reg_loss = ...\n",
        "    heading_class_loss = ...\n",
        "    heading_reg_loss = ...\n",
        "    regularizer_loss = ...\n",
        "\n",
        "    # scale loss on custom value if needed\n",
        "    heading_reg_loss = heading_reg_loss * 20\n",
        "    size_reg_loss = size_reg_loss * 3 * 20\n",
        "    return center_loss, size_class_loss, size_reg_loss, heading_class_loss, heading_reg_loss, regularizer_loss\n",
        "\n",
        "\n",
        "def resample_cloud(cloud, positive_points_mask, num_samples=OBJECT_CLOUD_SZ):\n",
        "    positive_points_mask = positive_points_mask.float() * 150\n",
        "    object_idx = torch.multinomial(\n",
        "        nn.Softmax(dim=1)(positive_points_mask),\n",
        "        num_samples=num_samples,\n",
        "        replacement=True)[..., None].expand(-1, -1, cloud.shape[-1])\n",
        "    cloud = torch.gather(input=cloud, dim=1, index=object_idx)\n",
        "    return cloud\n",
        "\n",
        "def cloud_to_object_cloud(frustum_batch, segmentation_probas_batch):\n",
        "    \"\"\"\n",
        "    @return: tuple (new_frustrum_batch, object_cloud_centers)\n",
        "    where 'new_frustrum_batch' is copy of source batch with modified 'object_cloud'\n",
        "    (instead of 'cloud') and 'world_location' (if present) fields and\n",
        "    'object_cloud_centers' is tensor with mean points per sample\n",
        "\n",
        "    'object_cloud' needs to be resampled for every batch element to only include points that were segmented as\n",
        "    object points. New 'object_cloud' center is then subtracted from 'object_cloud' and 'world_location'\n",
        "    \"\"\"\n",
        "    # ===============YOUR CODE======================\n",
        "    # form new_frustrum_batch\n",
        "    # 1) copy data for fields ['size_idx', 'size_residual', 'heading_idx', 'heading_residual', 'kitti_scene_idx']\n",
        "    # from frustrum_batch\n",
        "    # 2) resample 'object_cloud' from 'cloud' field using function 'resample_cloud' and\n",
        "    # predicted segmentation mask as positive_points_mask. (you need to sample OBJECT_CLOUD_SZ points)\n",
        "    # 3) compute mean point (in 3D) for each batch sample, substract it from 'object_cloud' and 'world_location'\n",
        "    # keep the 4th coordinate (lidar intensity) unchanged\n",
        "\n",
        "    positive_points_mask = ...\n",
        "    object_points = ...\n",
        "    object_mean_points = ...\n",
        "    object_points_centered = ...\n",
        "\n",
        "    new_frustrum_batch = {\n",
        "        'object_cloud': object_points_centered,\n",
        "    }\n",
        "    if 'world_location' in frustum_batch:\n",
        "        new_frustrum_batch['world_location'] = frustum_batch['world_location'] - object_mean_points\n",
        "\n",
        "    for field in ['size_idx', 'size_residual', 'heading_idx', 'heading_residual', 'kitti_scene_idx']:\n",
        "        if field in frustum_batch:\n",
        "            new_frustrum_batch[field] = frustum_batch[field]\n",
        "    return new_frustrum_batch, object_mean_points\n",
        "\n",
        "\n",
        "def detection_output_to_center_dims_rot(detection_output):\n",
        "    center, size_class, size_reg, heading_class, heading_reg, trans = detection_output\n",
        "    #================YOUR CODE==================\n",
        "    # decode bounding box and return predicted center, sizes and heading\n",
        "    size_idx = ...\n",
        "    size_reg_for_pred_class = ...\n",
        "    sizes = g_mean_size_arr[size_idx] * (size_reg_for_pred_class + 1.0)\n",
        "\n",
        "    heading_idx = ...\n",
        "    heading_reg_for_pred_class = ...\n",
        "    angle_step = (2 * np.pi / NUM_HEADING_BIN)\n",
        "    heading = angle_step * (heading_idx + 0.5) + heading_reg_for_pred_class\n",
        "    return center.detach().cpu(), sizes, heading\n",
        "\n",
        "\n",
        "def visualize_detection_output(object_batch, detection_output, title):\n",
        "    plt.figure(figsize=(9, 9))\n",
        "    object_cloud = object_batch['object_cloud'][0].detach().cpu().numpy()\n",
        "    plt.scatter(object_cloud[..., 0], object_cloud[..., 1])\n",
        "    x_axis = 0\n",
        "    y_axis = 1\n",
        "\n",
        "    # GT\n",
        "    center, dims, rotation_y = detection_output_to_center_dims_rot((\n",
        "        object_batch['world_location'],\n",
        "        torch.nn.functional.one_hot(object_batch['size_idx'], 8),\n",
        "        object_batch['size_residual'],\n",
        "        torch.nn.functional.one_hot(object_batch['heading_idx'], 12),\n",
        "        object_batch['heading_residual'],\n",
        "        None))\n",
        "    gt_cuboid_pts = get_detection3d_corner_points(center[0], dims[0], rotation_y[0])\n",
        "    plt.scatter(gt_cuboid_pts[..., 0], gt_cuboid_pts[..., 1], color='red')\n",
        "    plt.axline(\n",
        "        (center[0][x_axis], center[0][y_axis]),\n",
        "        (center[0][x_axis] + np.cos(rotation_y[0]), center[0][y_axis] + np.sin(rotation_y[0])), color='red')\n",
        "    plt.title(title)\n",
        "\n",
        "    # Pred\n",
        "    center, dims, rotation_y = detection_output_to_center_dims_rot(detection_output)\n",
        "    cuboid_pts = get_detection3d_corner_points(center[0], dims[0], rotation_y[0])\n",
        "    plt.scatter(cuboid_pts[..., x_axis], cuboid_pts[..., y_axis], color='orange')\n",
        "    plt.axline(\n",
        "        (center[0][x_axis], center[0][y_axis]),\n",
        "        (center[0][x_axis] + np.cos(rotation_y[0]), center[0][y_axis] + np.sin(rotation_y[0])), color='orange')\n",
        "\n",
        "    plt.gca().set_aspect('equal', adjustable='box')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_detection_pointnet_one_epoch(detection_pointnet, segmentation_pointnet, train_data_generator,\n",
        "                                       optim, transform_regularizer_weight):\n",
        "    means = [RunningMean() for _ in range(6)]\n",
        "\n",
        "    detection_pointnet.train()\n",
        "    for batch_idx, batch in enumerate(train_data_generator):\n",
        "        optim.zero_grad()\n",
        "        batch = {x: y.cuda() for x, y in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            segmentation_output, _ = segmentation_pointnet(batch['cloud'].float())\n",
        "\n",
        "        object_batch, centers_offset = cloud_to_object_cloud(batch, segmentation_output)\n",
        "        detection_output = detection_pointnet(object_batch['object_cloud'].float())\n",
        "\n",
        "        detection_loss = calculate_detection_loss(object_batch, detection_output, transform_regularizer_weight)\n",
        "        combined_loss = torch.sum(torch.stack(detection_loss))\n",
        "        combined_loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        for i in range(6):\n",
        "            means[i].add(detection_loss[i].item())\n",
        "\n",
        "        print(f'Train batch: {batch_idx:4d}/{len(train_data_generator):-4d}    center: {means[0].get():.3f}, '\n",
        "              f'size_class: {means[1].get():.3f}, size_reg: {means[2].get():.3f}, heading_class: {means[3].get():.3f}, '\n",
        "              f'heading_reg: {means[4].get():.3f}, trans reg: {means[5].get():.3f}', end='\\r')\n",
        "    print()\n",
        "    return\n",
        "\n",
        "\n",
        "def eval_detection_model(detection_pointnet, segmentation_pointnet, val_data_generator, transform_regularizer_weight,\n",
        "                        visualize_sample=False):\n",
        "    means = [RunningMean() for _ in range(6)]\n",
        "    with torch.no_grad():\n",
        "        detection_pointnet.eval()\n",
        "        for batch_idx, batch in enumerate(val_data_generator):\n",
        "            batch = {x: y.cuda() for x, y in batch.items()}\n",
        "            segmentation_output, _ = segmentation_pointnet(batch['cloud'].float())\n",
        "            object_batch, centers_offset = cloud_to_object_cloud(batch, segmentation_output)\n",
        "            detection_output = detection_pointnet(object_batch['object_cloud'].float())\n",
        "\n",
        "            detection_loss = calculate_detection_loss(object_batch, detection_output, transform_regularizer_weight)\n",
        "            combined_loss = torch.sum(torch.stack(detection_loss))\n",
        "            for i in range(6):\n",
        "                means[i].add(detection_loss[i].item())\n",
        "\n",
        "            print(f'Valid batch: {batch_idx:4d}/{len(val_data_generator):-4d}    center: {means[0].get():.3f}, '\n",
        "                  f'size_class: {means[1].get():.3f}, size_reg: {means[2].get():.3f}, heading_class: {means[3].get():.3f}, '\n",
        "                  f'heading_reg: {means[4].get():.3f}, trans reg: {means[5].get():.3f}', end='\\r')\n",
        "    print()\n",
        "\n",
        "    if visualize_sample:\n",
        "        visualize_detection_output(object_batch, detection_output, f'Val sample result')\n",
        "\n",
        "    mean_loss = sum(elem.get() for elem in means)\n",
        "\n",
        "    return mean_loss\n",
        "\n",
        "\n",
        "def train_detection_pointnet(detection_pointnet, segmentation_pointnet, frustum_train, frustum_val,\n",
        "                             batch_size=16, n_epochs=90,\n",
        "                             transform_regularizer_weight=1e-3):\n",
        "    train_data_generator = data.DataLoader(frustum_train, batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    val_data_generator = data.DataLoader(frustum_val, batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "    optim = torch.optim.Adam([\n",
        "            {'params': detection_pointnet.parameters()}\n",
        "        ], lr=1e-3, weight_decay=1e-6)\n",
        "    lr_sched = torch.optim.lr_scheduler.StepLR(optim, step_size=30, gamma=0.2)\n",
        "\n",
        "    best_model_loss = None\n",
        "\n",
        "    segmentation_pointnet.eval()\n",
        "    for epoch in range(n_epochs):\n",
        "        print('Epoch:', epoch)\n",
        "        train_detection_pointnet_one_epoch(detection_pointnet, segmentation_pointnet, train_data_generator,\n",
        "                                           optim, transform_regularizer_weight)\n",
        "\n",
        "        visualize_sample = epoch >= 30 and epoch % 10 == 0\n",
        "\n",
        "        model_loss = eval_detection_model(detection_pointnet, segmentation_pointnet, val_data_generator,\n",
        "                                          transform_regularizer_weight,\n",
        "                                          visualize_sample)\n",
        "\n",
        "        if best_model_loss is None or model_loss < best_model_loss:\n",
        "            best_model_loss = model_loss\n",
        "            torch.save(detection_pointnet.state_dict(), 'detection.pth')\n",
        "            print('new best model is saved')\n",
        "        print()\n",
        "        lr_sched.step()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29816a4-4d09-4181-b53c-73749600270e",
      "metadata": {
        "scrolled": true,
        "id": "c29816a4-4d09-4181-b53c-73749600270e"
      },
      "outputs": [],
      "source": [
        "detection_pointnet = PointNetDetector().cuda()\n",
        "train_detection_pointnet(detection_pointnet, segmentation_pointnet, frustum_train, frustum_val, batch_size=16, n_epochs=90,\n",
        "                        transform_regularizer_weight=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d68e75c-9949-4c15-b700-d81265bd84a8",
      "metadata": {
        "id": "0d68e75c-9949-4c15-b700-d81265bd84a8"
      },
      "outputs": [],
      "source": [
        "detection_pointnet = PointNetDetector().cuda()\n",
        "detection_pointnet.load_state_dict(torch.load('detection.pth'))\n",
        "detection_pointnet.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1bceba9-efc3-4528-8eff-170c1ac59caa",
      "metadata": {
        "id": "e1bceba9-efc3-4528-8eff-170c1ac59caa"
      },
      "source": [
        "Посчитаем метрики на тестовом датасете"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffdb1765-bc92-4585-b595-f0a75aee934c",
      "metadata": {
        "id": "ffdb1765-bc92-4585-b595-f0a75aee934c"
      },
      "outputs": [],
      "source": [
        "from metrics import compute_confusion_mtx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b76171-d5e9-4e11-8ac3-61cd5009a6de",
      "metadata": {
        "id": "e6b76171-d5e9-4e11-8ac3-61cd5009a6de"
      },
      "outputs": [],
      "source": [
        "test_kitti = KittiDataset(KITTI_ROOT, split='testing', only_easy=True)\n",
        "frustum_test = FrustumDataset(test_kitti, detector_2d_wrapper, Projector(), cuda=True)\n",
        "test_loader = data.DataLoader(frustum_test, 1, shuffle=False, drop_last=False, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2962730b-eace-4862-bf78-a6631a339d2f",
      "metadata": {
        "id": "2962730b-eace-4862-bf78-a6631a339d2f"
      },
      "outputs": [],
      "source": [
        "evaluated_data = {}\n",
        "\n",
        "with torch.no_grad():\n",
        "    detection_pointnet.eval()\n",
        "    segmentation_pointnet.eval()\n",
        "    for batch_idx, batch in enumerate(tqdm.tqdm(test_loader)):\n",
        "        batch = {x: y.cuda() for x, y in batch.items()}\n",
        "        segmentation_output, _ = segmentation_pointnet(batch['cloud'].float())\n",
        "        object_batch, centers_offset = cloud_to_object_cloud(batch, segmentation_output)\n",
        "        detection_output = detection_pointnet(object_batch['object_cloud'].float())\n",
        "\n",
        "        # we learn size bin to be equal to GT class, hence using it here\n",
        "        size_idx = torch.argmax(detection_output[1].cpu(), dim=1).detach().numpy()\n",
        "        center, dims, rotation_y = detection_output_to_center_dims_rot(detection_output)\n",
        "        center += centers_offset.cpu()\n",
        "        for frustum_idx in range(size_idx.shape[0]):\n",
        "            kitti_scene_idx = batch['kitti_scene_idx'][frustum_idx]\n",
        "            cuboid_pts = get_detection3d_corner_points(center[frustum_idx], dims[frustum_idx], rotation_y[frustum_idx])\n",
        "            frustum_rotation = FrustumDataset._rot_44_by_angle_world(-batch['frustum_rotation_angle'][frustum_idx].cpu())\n",
        "\n",
        "            rotated_cuboid_pts = Projector.from_homogenous_coords(Projector.to_homogenous_coords(cuboid_pts) @ frustum_rotation.T)\n",
        "            evaluated_data.setdefault(int(kitti_scene_idx), []).append((rotated_cuboid_pts, size_idx[frustum_idx]))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25839866-eb14-47d2-a303-d5cbb2f80ac6",
      "metadata": {
        "id": "25839866-eb14-47d2-a303-d5cbb2f80ac6"
      },
      "outputs": [],
      "source": [
        "from kitti_dataset import g_class2type\n",
        "\n",
        "conf = compute_confusion_mtx(frustum_test.kitti_dataset, evaluated_data, Projector())\n",
        "\n",
        "for cls_idx, cls_name in g_class2type.items():\n",
        "    cm = conf[cls_idx]\n",
        "    print(f'{cls_name:15s}: tp={cm[\"tp\"]:4d}, fp={cm[\"fp\"]:4d}, fn={cm[\"fn\"]:4d}, '\n",
        "          f'precision={cm[\"tp\"] / (cm[\"tp\"] + cm[\"fp\"]):.3f}, recall={cm[\"tp\"] / (cm[\"tp\"] + cm[\"fn\"]):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53db2ce7-d49f-4d0f-9302-da4216e2fd03",
      "metadata": {
        "id": "53db2ce7-d49f-4d0f-9302-da4216e2fd03"
      },
      "source": [
        "## 5. Визуализация того, что получилось\n",
        "### Предсказания для одного фруструма в 3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2137d722-9336-4ea1-9a57-b5943a869026",
      "metadata": {
        "id": "2137d722-9336-4ea1-9a57-b5943a869026"
      },
      "outputs": [],
      "source": [
        "from plotly.offline import init_notebook_mode\n",
        "from plotly.offline import iplot\n",
        "from plotly.offline import plot\n",
        "from plotly import graph_objs as go\n",
        "\n",
        "\n",
        "def plotly_add_cloud(fig, cloud, color, colorscale=None, name=None, showscale=None, cmin=None, cmax=None):\n",
        "    fig.add_scatter3d(x=cloud[..., 0], y=cloud[..., 1], z=cloud[..., 2], name=name, mode='markers',\n",
        "        marker=dict(size=2, color=color, colorscale=colorscale, showscale=showscale, cmin=cmin, cmax=cmax))\n",
        "\n",
        "def plotly_add_cuboid(fig, points, color, opacity=0.4):\n",
        "    fig.add_mesh3d(\n",
        "        # 8 vertices of a cube\n",
        "        x=points[:, 0],\n",
        "        y=points[:, 1],\n",
        "        z=points[:, 2],\n",
        "\n",
        "        i = [6, 0, 0, 0, 4, 4, 7, 7, 4, 0, 2, 3],\n",
        "        j = [2, 4, 1, 3, 5, 7, 5, 3, 0, 1, 7, 2],\n",
        "        k = [0, 6, 3, 2, 7, 6, 1, 1, 5, 5, 6, 7],\n",
        "        opacity=opacity,\n",
        "        color=color,\n",
        "        flatshading = True\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54c9380-aaca-4b8e-935e-7e51c8c3183c",
      "metadata": {
        "id": "e54c9380-aaca-4b8e-935e-7e51c8c3183c"
      },
      "outputs": [],
      "source": [
        "# NOTE: val frustum here!\n",
        "test_loader = data.DataLoader(frustum_val, 1, shuffle=True, drop_last=True, pin_memory=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_batch = next(iter(test_loader))\n",
        "    test_batch = {x: y.cuda() for x, y in test_batch.items()}\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74741a77-593b-4943-8aaa-2a20f502b0ce",
      "metadata": {
        "id": "74741a77-593b-4943-8aaa-2a20f502b0ce"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    segmentation_pointnet.eval()\n",
        "    segmentation_output = segmentation_pointnet(test_batch['cloud'].float())\n",
        "    object_batch, *_ = cloud_to_object_cloud(test_batch, segmentation_output[0])\n",
        "    detection_pointnet.eval()\n",
        "    detection_output = detection_pointnet(object_batch['object_cloud'].float())\n",
        "\n",
        "# GT\n",
        "center, dims, rotation_y = detection_output_to_center_dims_rot((\n",
        "    object_batch['world_location'],\n",
        "    torch.nn.functional.one_hot(object_batch['size_idx'], 8),\n",
        "    object_batch['size_residual'],\n",
        "    torch.nn.functional.one_hot(object_batch['heading_idx'], 12),\n",
        "    object_batch['heading_residual'],\n",
        "    None))\n",
        "gt_cuboid_pts = get_detection3d_corner_points(center[0], dims[0], rotation_y[0])\n",
        "\n",
        "# Pred\n",
        "center, dims, rotation_y = detection_output_to_center_dims_rot(detection_output)\n",
        "\n",
        "fig = go.Figure(layout=dict(scene=dict(\n",
        "        aspectmode='data',\n",
        "        xaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "        yaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "        zaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "    ), plot_bgcolor='#000', paper_bgcolor='black'))\n",
        "pred_cuboid_pts = get_detection3d_corner_points(center[0], dims[0], rotation_y[0])\n",
        "\n",
        "\n",
        "plotly_add_cuboid(fig, pred_cuboid_pts, 'red', 0.3)\n",
        "plotly_add_cuboid(fig, gt_cuboid_pts, 'green', 0.3)\n",
        "\n",
        "object_cloud = object_batch['object_cloud'].detach().cpu().numpy()\n",
        "\n",
        "plotly_add_cloud(fig, object_cloud[0], object_cloud[0, ..., -1], colorscale='Reds', cmin=0, cmax=1, name='Velodyne')\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63b83c7-be0c-4d95-82c5-8d16490226d6",
      "metadata": {
        "id": "c63b83c7-be0c-4d95-82c5-8d16490226d6"
      },
      "source": [
        "### Предсказания для сцены"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576121c6-6c61-4149-8b17-3f22f87bfa7b",
      "metadata": {
        "scrolled": true,
        "id": "576121c6-6c61-4149-8b17-3f22f87bfa7b"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(layout=dict(scene=dict(\n",
        "        aspectmode='data',\n",
        "        xaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "        yaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "        zaxis=dict(showbackground=False, gridcolor=\"rgba(0, 0, 0, 0)\"),\n",
        "    ), plot_bgcolor='#000', paper_bgcolor='black', height=1000))\n",
        "\n",
        "kitti_idx = 15\n",
        "cloud = frustum_test.kitti_dataset[kitti_idx]['cloud']\n",
        "\n",
        "plotly_add_cloud(fig, cloud, cloud[..., -1], colorscale='Blues', cmin=0, cmax=1, name='Velodyne')\n",
        "for obj_cuboid, obj_cls in evaluated_data.get(kitti_idx, []):\n",
        "    plotly_add_cuboid(fig, obj_cuboid, 'red', 0.3)\n",
        "\n",
        "for label in frustum_test.kitti_dataset[kitti_idx]['labels']:\n",
        "    # if label['type'] == 'DontCare':\n",
        "    #     continue\n",
        "    if label['dimensions'][0] < 0:\n",
        "        continue\n",
        "    world_location = Projector.from_homogenous_coords(\n",
        "        Projector.camera_to_world(np.array(label['location']), frustum_test.kitti_dataset[kitti_idx]['calibration']))\n",
        "    cuboid_pts = get_detection3d_corner_points(\n",
        "        torch.tensor(world_location),\n",
        "        torch.tensor(label['dimensions']),\n",
        "        np.pi / 2 - label['rotation_y'])\n",
        "    # cuboid_pts = Projector.camera_to_world(cuboid_pts, frustum_val.kitti_dataset[kitti_idx]['calibration'])\n",
        "    plotly_add_cuboid(fig, cuboid_pts, 'yellow' if label['type'] == 'DontCare' else 'green', 0.3)\n",
        "\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.imshow(torch.moveaxis(frustum_test.kitti_dataset[kitti_idx]['image'], 0, -1))\n",
        "plt.show()\n",
        "\n",
        "iplot(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2083fbf0-505c-4607-b519-8e9cff7eb8a7",
      "metadata": {
        "scrolled": true,
        "id": "2083fbf0-505c-4607-b519-8e9cff7eb8a7"
      },
      "outputs": [],
      "source": [
        "frustum_test.kitti_dataset[kitti_idx]['labels']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e134a29-ee4e-4a0b-80d4-f20181392b9f",
      "metadata": {
        "id": "4e134a29-ee4e-4a0b-80d4-f20181392b9f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}